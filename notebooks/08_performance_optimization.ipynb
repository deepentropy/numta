{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization\n",
    "\n",
    "This notebook demonstrates how to optimize numta performance using different backends and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numta import SMA, EMA, RSI, get_available_backends, HAS_NUMBA\n",
    "from numta import SMA_auto, SMA_cumsum\n",
    "from numta.benchmark import PerformanceMeasurement\n",
    "\n",
    "print(f\"Numba available: {HAS_NUMBA}\")\n",
    "print(f\"Available backends: {get_available_backends()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend Selection\n",
    "\n",
    "numta supports multiple computation backends:\n",
    "\n",
    "| Backend | Speed | Requirements | Notes |\n",
    "|---------|-------|--------------|-------|\n",
    "| numpy (default) | 1x | None | Pure NumPy, works everywhere |\n",
    "| cumsum | ~3x | None | Optimized algorithm, no dependencies |\n",
    "| numba | 5-10x | numba | JIT compilation for best performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Backend (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "np.random.seed(42)\n",
    "data = np.random.uniform(100, 200, 10000)\n",
    "\n",
    "# Default NumPy implementation\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    result = SMA(data, timeperiod=30)\n",
    "numpy_time = (time.perf_counter() - start) / 100\n",
    "\n",
    "print(f\"NumPy backend: {numpy_time*1000:.3f} ms per call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumsum Optimization\n",
    "\n",
    "The cumsum backend uses an optimized algorithm that's ~3x faster than the default implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumsum optimized implementation\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    result = SMA_cumsum(data, timeperiod=30)\n",
    "cumsum_time = (time.perf_counter() - start) / 100\n",
    "\n",
    "print(f\"Cumsum backend: {cumsum_time*1000:.3f} ms per call\")\n",
    "print(f\"Speedup: {numpy_time/cumsum_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba JIT Compilation\n",
    "\n",
    "Numba provides the best performance through Just-In-Time compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_NUMBA:\n",
    "    from numta import SMA_numba\n",
    "    \n",
    "    # Warmup (first call triggers compilation)\n",
    "    _ = SMA_numba(data, timeperiod=30)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        result = SMA_numba(data, timeperiod=30)\n",
    "    numba_time = (time.perf_counter() - start) / 100\n",
    "    \n",
    "    print(f\"Numba backend: {numba_time*1000:.3f} ms per call\")\n",
    "    print(f\"Speedup vs NumPy: {numpy_time/numba_time:.2f}x\")\n",
    "    print(f\"Speedup vs cumsum: {cumsum_time/numba_time:.2f}x\")\n",
    "else:\n",
    "    print(\"Numba not available. Install with: pip install numba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Backend Selection\n",
    "\n",
    "Use `SMA_auto` to automatically select the best available backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto backend selection\n",
    "result = SMA_auto(data, timeperiod=30, backend='auto')\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Or specify a specific backend\n",
    "result_cumsum = SMA_auto(data, timeperiod=30, backend='cumsum')\n",
    "print(f\"Cumsum result shape: {result_cumsum.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "numta includes a built-in benchmarking tool for comparing implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup benchmark\n",
    "bench = PerformanceMeasurement()\n",
    "\n",
    "# Add implementations to compare\n",
    "bench.add_function(\"NumPy SMA\", SMA, data, timeperiod=30)\n",
    "bench.add_function(\"Cumsum SMA\", SMA_cumsum, data, timeperiod=30)\n",
    "\n",
    "if HAS_NUMBA:\n",
    "    from numta import SMA_numba\n",
    "    bench.add_function(\"Numba SMA\", SMA_numba, data, timeperiod=30)\n",
    "\n",
    "# Run benchmark\n",
    "results = bench.run(iterations=100, warmup=10)\n",
    "bench.print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Across Data Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across different data sizes\n",
    "sizes = [1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "print(\"Performance by data size (ms):\\n\")\n",
    "print(f\"{'Size':>10} | {'NumPy':>10} | {'Cumsum':>10}\", end=\"\")\n",
    "if HAS_NUMBA:\n",
    "    print(f\" | {'Numba':>10}\")\n",
    "else:\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for size in sizes:\n",
    "    test_data = np.random.uniform(100, 200, size)\n",
    "    \n",
    "    # NumPy\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(50):\n",
    "        SMA(test_data, timeperiod=30)\n",
    "    np_time = (time.perf_counter() - start) / 50 * 1000\n",
    "    \n",
    "    # Cumsum\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(50):\n",
    "        SMA_cumsum(test_data, timeperiod=30)\n",
    "    cs_time = (time.perf_counter() - start) / 50 * 1000\n",
    "    \n",
    "    print(f\"{size:>10} | {np_time:>10.3f} | {cs_time:>10.3f}\", end=\"\")\n",
    "    \n",
    "    if HAS_NUMBA:\n",
    "        from numta import SMA_numba\n",
    "        _ = SMA_numba(test_data, timeperiod=30)  # warmup\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(50):\n",
    "            SMA_numba(test_data, timeperiod=30)\n",
    "        nb_time = (time.perf_counter() - start) / 50 * 1000\n",
    "        print(f\" | {nb_time:>10.3f}\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization\n",
    "\n",
    "Tips for reducing memory usage with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Appropriate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float64 (default) - 8 bytes per element\n",
    "data_64 = np.random.uniform(100, 200, 1000000).astype(np.float64)\n",
    "print(f\"float64 size: {data_64.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# float32 - 4 bytes per element (often sufficient)\n",
    "data_32 = data_64.astype(np.float32)\n",
    "print(f\"float32 size: {data_32.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(data, chunk_size=100000, timeperiod=30):\n",
    "    \"\"\"Process large datasets in chunks to reduce memory usage.\"\"\"\n",
    "    results = []\n",
    "    overlap = timeperiod - 1\n",
    "    \n",
    "    for i in range(0, len(data), chunk_size - overlap):\n",
    "        chunk = data[i:i + chunk_size]\n",
    "        sma_chunk = SMA(chunk, timeperiod=timeperiod)\n",
    "        \n",
    "        if i == 0:\n",
    "            results.append(sma_chunk)\n",
    "        else:\n",
    "            # Skip overlap region\n",
    "            results.append(sma_chunk[overlap:])\n",
    "    \n",
    "    return np.concatenate(results)\n",
    "\n",
    "# Test chunked processing\n",
    "large_data = np.random.uniform(100, 200, 500000)\n",
    "result = process_in_chunks(large_data)\n",
    "print(f\"Processed {len(large_data)} points -> {len(result)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Streaming for Real-Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numta.streaming import StreamingSMA\n",
    "\n",
    "# Streaming uses O(timeperiod) memory instead of O(data_length)\n",
    "sma = StreamingSMA(timeperiod=20)\n",
    "\n",
    "# Process one point at a time\n",
    "for price in large_data[:100]:\n",
    "    sma.update(price)\n",
    "\n",
    "print(f\"StreamingSMA uses minimal memory regardless of data length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-allocate Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Growing arrays\n",
    "def slow_multi_indicator(data):\n",
    "    results = {}\n",
    "    results['sma'] = SMA(data, timeperiod=20)\n",
    "    results['ema'] = EMA(data, timeperiod=20)\n",
    "    results['rsi'] = RSI(data, timeperiod=14)\n",
    "    return results\n",
    "\n",
    "# Good: Pre-allocate all at once\n",
    "def fast_multi_indicator(data):\n",
    "    n = len(data)\n",
    "    results = np.empty((3, n), dtype=np.float64)\n",
    "    results[0] = SMA(data, timeperiod=20)\n",
    "    results[1] = EMA(data, timeperiod=20)\n",
    "    results[2] = RSI(data, timeperiod=14)\n",
    "    return results\n",
    "\n",
    "# Compare\n",
    "test = np.random.uniform(100, 200, 100000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "_ = slow_multi_indicator(test)\n",
    "slow_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "_ = fast_multi_indicator(test)\n",
    "fast_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Dict approach: {slow_time*1000:.2f} ms\")\n",
    "print(f\"Array approach: {fast_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Avoid Repeated Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Recalculating for each use\n",
    "def bad_strategy(data):\n",
    "    signal1 = SMA(data, 20) > SMA(data, 50)\n",
    "    signal2 = SMA(data, 20) > data  # Recalculates SMA!\n",
    "    return signal1 & signal2\n",
    "\n",
    "# Good: Calculate once, reuse\n",
    "def good_strategy(data):\n",
    "    sma20 = SMA(data, 20)\n",
    "    sma50 = SMA(data, 50)\n",
    "    signal1 = sma20 > sma50\n",
    "    signal2 = sma20 > data\n",
    "    return signal1 & signal2\n",
    "\n",
    "# Compare\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    bad_strategy(test)\n",
    "bad_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    good_strategy(test)\n",
    "good_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Bad approach: {bad_time*1000:.2f} ms\")\n",
    "print(f\"Good approach: {good_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {bad_time/good_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Contiguous Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-contiguous array (e.g., from slicing)\n",
    "data_nc = test[::2]  # Every other element\n",
    "print(f\"Contiguous: {data_nc.flags['C_CONTIGUOUS']}\")\n",
    "\n",
    "# Make contiguous\n",
    "data_c = np.ascontiguousarray(data_nc)\n",
    "print(f\"Now contiguous: {data_c.flags['C_CONTIGUOUS']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Tip | Impact |\n",
    "|-----|--------|\n",
    "| Use Numba backend | 5-10x faster |\n",
    "| Use cumsum backend | ~3x faster |\n",
    "| Pre-allocate arrays | Reduces memory fragmentation |\n",
    "| Cache repeated calculations | Avoids redundant work |\n",
    "| Use streaming for real-time | Constant memory usage |\n",
    "| Process in chunks | Handles large datasets |\n",
    "| Use float32 if precision allows | 50% memory reduction |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
