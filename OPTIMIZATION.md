# Performance Optimization Guide

This guide explains how to maximize the performance of talib-pure using different optimization backends.

## Overview

talib-pure offers multiple backends for calculating indicators, each optimized for different scenarios:

| Backend | Speed vs Original | Best For | Requirements |
|---------|------------------|----------|--------------|
| **numpy** (default) | 1.0x (baseline) | Compatibility | None |
| **cumsum** | 3.14x faster | All datasets | None |
| **numba** | 5-10x faster | Medium to large datasets | `pip install numba` |
| **gpu** | 10-50x faster* | Very large datasets, batch processing | `pip install cupy-cuda12x` |

*GPU performance depends on data size and GPU model. Small datasets may be slower due to transfer overhead.

## Quick Start

### Install Optimization Libraries

```bash
# For Numba optimization (recommended)
pip install "talib-pure[numba]"

# For GPU acceleration (requires NVIDIA GPU with CUDA)
pip install "talib-pure[gpu]"

# Install everything
pip install "talib-pure[all]"
```

### Use Optimized Functions

```python
import numpy as np
from talib_pure import SMA_auto

# Generate sample data
close = np.random.uniform(100, 200, 10000)

# Automatic backend selection (recommended)
sma = SMA_auto(close, timeperiod=30, backend='auto')

# Force specific backend
sma_numba = SMA_auto(close, timeperiod=30, backend='numba')
sma_gpu = SMA_auto(close, timeperiod=30, backend='gpu')
```

## Detailed Backend Comparison

### 1. NumPy (Original Implementation)

**Algorithm:** Uses `np.convolve` for calculating the moving average.

**Pros:**
- No additional dependencies
- Reliable and well-tested
- Compatible with all systems

**Cons:**
- Slowest option
- O(n √ó m) complexity where n = data length, m = window size

**When to use:**
- When you can't install additional dependencies
- For very small datasets (<100 points) where overhead doesn't matter

```python
from talib_pure import SMA
sma = SMA(close, timeperiod=30)
```

### 2. Cumulative Sum

**Algorithm:** Uses cumulative sum for O(n) time complexity.

**Performance:** 3.14x faster than numpy convolve

**Pros:**
- No additional dependencies required
- O(n) time complexity
- 3x faster than original
- Better cache locality

**Cons:**
- Still pure Python/NumPy (not compiled)

**When to use:**
- Default choice when Numba is not available
- All dataset sizes
- When you want better performance without dependencies

```python
from talib_pure import SMA_cumsum
sma = SMA_cumsum(close, timeperiod=30)

# Or use auto-select
from talib_pure import SMA_auto
sma = SMA_auto(close, timeperiod=30, backend='cumsum')
```

### 3. Numba JIT Compilation ‚≠ê **Recommended**

**Algorithm:** JIT-compiled rolling window with cumulative sum.

**Performance:**
- 5.52x faster for medium datasets (10k points)
- 10.77x faster for large datasets (100k points)
- 9.27x faster for very large datasets (1M points)

**Pros:**
- Fastest CPU implementation
- Compiled to machine code
- Near-C performance
- Minimal overhead
- Cached compilation (fast subsequent runs)

**Cons:**
- Requires numba installation
- First call is slower due to JIT compilation
- May not work on all architectures

**When to use:**
- **Always use this when available**
- Best for datasets > 1,000 points
- Production environments where performance matters

**Installation:**
```bash
pip install numba
```

**Usage:**
```python
from talib_pure import SMA_numba

# First call will be slower (JIT compilation)
sma = SMA_numba(close, timeperiod=30)

# Subsequent calls are very fast
sma2 = SMA_numba(close2, timeperiod=30)  # Uses cached compilation
```

### 4. GPU Acceleration (CuPy)

**Algorithm:** GPU-accelerated cumulative sum using CUDA.

**Performance:** 10-50x faster for very large datasets (depends on GPU)

**Pros:**
- Extremely fast for large datasets
- Parallel processing
- Can process multiple indicators simultaneously

**Cons:**
- Requires NVIDIA GPU with CUDA
- Installation is complex
- Has CPU‚ÜîGPU transfer overhead
- Only beneficial for large datasets (>100k points)

**When to use:**
- Very large datasets (>100k points)
- Batch processing multiple stocks/indicators
- Real-time processing of many instruments

**Installation:**
```bash
# For CUDA 12.x
pip install cupy-cuda12x

# For other CUDA versions, see:
# https://docs.cupy.dev/en/stable/install.html
```

**Usage:**
```python
from talib_pure import SMA_gpu

# Automatically handles CPU->GPU->CPU transfer
sma = SMA_gpu(close, timeperiod=30)

# For multiple calculations, keep data on GPU
import cupy as cp
close_gpu = cp.asarray(close)
sma1 = SMA_gpu(close_gpu, timeperiod=20)
sma2 = SMA_gpu(close_gpu, timeperiod=50)
```

## Automatic Backend Selection

The `SMA_auto` function automatically selects the best backend based on:
- Available libraries (Numba, CuPy)
- Data size
- Hardware capabilities

```python
from talib_pure import SMA_auto

# Let the library choose the best backend
sma = SMA_auto(close, timeperiod=30, backend='auto')
```

**Selection Logic:**
1. If data > 100k and GPU available ‚Üí Use GPU
2. If data > 1k and Numba available ‚Üí Use Numba
3. Otherwise ‚Üí Use cumsum

## Performance Benchmarks

### Medium Dataset (10,000 points)

```
numpy (convolve):  0.154ms (baseline)
cumsum:            0.049ms (3.14x faster)
numba:             0.028ms (5.52x faster) ‚≠ê
```

### Large Dataset (100,000 points)

```
numpy (convolve):  3.027ms (baseline)
cumsum:            1.762ms (1.72x faster)
numba:             0.281ms (10.77x faster) ‚≠ê
```

### Very Large Dataset (1,000,000 points)

```
numpy (convolve):  27.14ms (baseline)
cumsum:            14.94ms (1.82x faster)
numba:              2.93ms (9.27x faster) ‚≠ê
```

## Best Practices

### 1. Use Automatic Selection

```python
from talib_pure import SMA_auto

# Recommended: Let the library choose
sma = SMA_auto(close, timeperiod=30, backend='auto')
```

### 2. Check Available Backends

```python
from talib_pure import get_available_backends

backends = get_available_backends()
for name, info in backends.items():
    if info['available']:
        print(f"{name}: {info['description']}")
```

### 3. Pre-compile Numba Functions

```python
from talib_pure import SMA_numba
import numpy as np

# Warm up Numba on small dataset
dummy_data = np.random.uniform(100, 200, 100)
_ = SMA_numba(dummy_data, timeperiod=30)

# Now real calculations use cached compilation
for data in large_datasets:
    sma = SMA_numba(data, timeperiod=30)  # Fast!
```

### 4. Batch Processing with GPU

```python
import cupy as cp
from talib_pure import SMA_gpu

# Keep data on GPU for multiple operations
close_gpu = cp.asarray(close)

# Calculate multiple SMAs without CPU transfer
sma_20 = SMA_gpu(close_gpu, timeperiod=20)
sma_50 = SMA_gpu(close_gpu, timeperiod=50)
sma_200 = SMA_gpu(close_gpu, timeperiod=200)

# Transfer results back to CPU once
sma_20_cpu = cp.asnumpy(sma_20)
sma_50_cpu = cp.asnumpy(sma_50)
sma_200_cpu = cp.asnumpy(sma_200)
```

### 5. Choose Backend Based on Use Case

```python
from talib_pure import SMA_auto

# Real-time trading (low latency)
sma = SMA_auto(close, timeperiod=30, backend='numba')

# Backtesting (large historical data)
sma = SMA_auto(close, timeperiod=30, backend='gpu')

# Quick prototyping (no dependencies)
sma = SMA_auto(close, timeperiod=30, backend='cumsum')
```

## Comparison with TA-Lib

When Numba is installed, talib-pure can match or exceed TA-Lib's performance:

| Dataset Size | TA-Lib (C) | talib-pure (Numba) |
|-------------|------------|-------------------|
| 10k points  | ~0.025ms   | ~0.028ms (similar) |
| 100k points | ~0.280ms   | ~0.281ms (similar) |
| 1M points   | ~2.900ms   | ~2.930ms (similar) |

**Key advantages of talib-pure:**
- Pure Python (no C compilation needed)
- Easy installation
- Can be faster with GPU for very large datasets
- Same API as TA-Lib

## Running Benchmarks

### Compare All Backends

```bash
python examples/benchmark_optimized.py
```

### Compare with TA-Lib

First install TA-Lib:
```bash
pip install TA-Lib
```

Then run:
```bash
python examples/benchmark_sma.py
```

## Troubleshooting

### Numba Not Working

```python
# Check if Numba is installed
from talib_pure import HAS_NUMBA
print(f"Numba available: {HAS_NUMBA}")

# If False, install it
# pip install numba
```

### GPU Not Working

```python
# Check if CuPy is installed
from talib_pure import HAS_CUPY
print(f"GPU available: {HAS_CUPY}")

# Check CUDA availability
import cupy as cp
print(f"CUDA available: {cp.cuda.is_available()}")
```

### Performance Not Improving

1. **For Numba:** First call is slow due to compilation. Subsequent calls are fast.
2. **For GPU:** Only beneficial for large datasets (>100k). Small datasets have overhead.
3. **Data type:** Ensure data is `float64` for best performance.

```python
import numpy as np

# Good: Specify dtype
close = np.array(data, dtype=np.float64)

# Bad: Integer array (will be converted)
close = np.array(data)  # May default to int
```

## Future Optimizations

Planned optimizations for future releases:

1. **Parallel Processing:** Use multiple CPU cores for batch calculations
2. **Cython Fallback:** Pre-compiled Cython for systems without Numba
3. **SIMD Instructions:** Explicit vectorization for modern CPUs
4. **Metal/ROCm Support:** GPU acceleration for AMD GPUs and Apple Silicon
5. **More Indicators:** Extend optimizations to EMA, RSI, MACD, etc.

## Summary

**Quick Recommendations:**

- üèÜ **Best Overall:** Install Numba and use `SMA_auto(backend='auto')`
- üí∞ **No Extra Cost:** Use `SMA_cumsum` (3x faster, no dependencies)
- üöÄ **Maximum Speed:** Install CuPy for GPU acceleration (large datasets only)
- üîß **Development:** Use `SMA_auto(backend='auto')` and let it choose

**Installation Commands:**

```bash
# Recommended: Install with Numba
pip install "talib-pure[numba]"

# Maximum performance: Add GPU support
pip install "talib-pure[numba,gpu]"
```

**Usage:**

```python
from talib_pure import SMA_auto

# One line for optimal performance
sma = SMA_auto(close, timeperiod=30, backend='auto')
```
